{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc59f005",
   "metadata": {},
   "source": [
    "KEY DATES LOG:\n",
    "\n",
    "5/7 - got next.js hello world working on custom domain with ec2\n",
    "11/7 - set up the frontend server so that it autostarts the webpage when launched, and keeps the same IP address "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f3e8d",
   "metadata": {},
   "source": [
    "Previous experiment with training LoRAs weren't possible on the Mac, so just going to run a regular stable diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b5603",
   "metadata": {},
   "source": [
    "Based on advice from ChatGPT and my own research:\n",
    "\n",
    "- Next.js as it is a full package and is optimal for SEO\n",
    "- FastAPI (Python) for backend IF NEEDED due to familiarity and it's a modern technology\n",
    "- TailwindCSS for styling\n",
    "- EC2 for running the AI model (Stable Diffusion)\n",
    "- Redis for queuing\n",
    "- DynamoDB for general storage NEED A SQL DATABASE TOO\n",
    "- Firebase Auth + Stripe for authorisation and billing\n",
    "- CloudFront for CDN (or is this already integrated into the next.js hoster)\n",
    "- ComfyUI for generating images\n",
    "- GitHub Actions for CI/CD\n",
    "- RDS for credits data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1291b66",
   "metadata": {},
   "source": [
    "Next.js setup:\n",
    "\n",
    "npx create-next-app@latest my-ai-app\n",
    "cd my-ai-app\n",
    "\n",
    "say yes to all options\n",
    "\n",
    "npm run dev, to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255a389",
   "metadata": {},
   "source": [
    "hosting on ec2:\n",
    "\n",
    "launch a micro ec2 instance\n",
    "ssh into it (run in bash: chmod 400 /Users/klioballiu/Desktop/RollerAI/rollerai_key.pem then  ssh -i \"rollerai_key.pem\" ubuntu@ec2-18-175-197-226.eu-west-2.compute.amazonaws.com),\n",
    " install node and git, run the build\n",
    " curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
    "sudo apt-get install -y nodejs\n",
    "sudo apt install git\n",
    "git clone https://github.com/your-username/your-repo.git\n",
    "cd to relevant folder...\n",
    "npm install\n",
    "npm run build\n",
    "npm start\n",
    "\n",
    "setup reverse proxy using nginx and domain (open relevant ports and point domain to ec2 with an A record), and secure with https using 'lets encrypt'\n",
    "\n",
    "pm2 is used to keep the app running and doesn't 'block' the ssh line\n",
    "\n",
    "I had the right feeling that adding 3000 to inbound rules is not sound security-wise. Instead we use reverse proxy to reflect it onto the protected ports\n",
    "\n",
    "need to do elastic IP on the front end EC2, but I'll do it when it's needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e06df",
   "metadata": {},
   "source": [
    "[User Web App]\n",
    "     |\n",
    "     v\n",
    "[Frontend (EC2 1) — Next.js + Auth + Stripe]\n",
    "     |\n",
    "     v\n",
    "[Redis Queue (hosted or self-managed)]\n",
    "     |\n",
    "     v\n",
    "[Worker Server (EC2 2) — ComfyUI/Auto1111 + Stable Diffusion]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f5d9e",
   "metadata": {},
   "source": [
    "User Request\n",
    "   ↓\n",
    "Web Server (e.g., Next.js + API)\n",
    "   ↓\n",
    "Redis Queue  ← Monitor (sees queue length or task time) + KURBENETES?\n",
    "   ↓\n",
    "Worker (on EC2) ← Autoscaler (spins more EC2s if needed) + KURBENETES?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d07e34",
   "metadata": {},
   "source": [
    "+--------------------+\n",
    "|  User Request/API  |\n",
    "+--------------------+\n",
    "          ↓\n",
    "+--------------------+         +--------------------+\n",
    "|   API Pod (FastAPI)|  → → →  |     Redis Pod      |\n",
    "+--------------------+         +--------------------+\n",
    "          ↓                            ↑\n",
    "   Enqueue image job          Store job in queue\n",
    "          ↓                            ↓\n",
    "+---------------------+   ← ← ←  +---------------------+\n",
    "|  Worker Pod (Celery,|         |  KEDA (Auto-Scaler)  |\n",
    "|  RQ, custom script) |         +---------------------+\n",
    "+---------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee9b7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "736628e0",
   "metadata": {},
   "source": [
    "1 - FastAPI endpoint \n",
    "2 - Worker script + image generator\n",
    "3 - Redis queue \n",
    "4 - Deploy on Kubernetes\n",
    "5 - Add KEDA + stress test & autoscale\n",
    "\n",
    "I ASSUME THIS WILL ALL RUN IN A 'virtual private cloud'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c7b41",
   "metadata": {},
   "source": [
    "Deciding to not do image to image as it's too complex to get working right. ControlNet is definitely needed as I tried an online img2img and even when fine-tuning the image weight the results were still rubbish.\n",
    "\n",
    "Also choosing SD1.5 as it is the lightest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a183e43",
   "metadata": {},
   "source": [
    "Turns out spot instances can actually last a few hours so they can be good for images - check their 'frequency of interruption'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7db84",
   "metadata": {},
   "source": [
    "Using PostgreSQL for data integrity for the credits system (use amazon RDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325faf19",
   "metadata": {},
   "source": [
    "Choosing 'diffusers' to run SD1.5 on the EC2 as GPT says it's the best for CLI and API pipelines.\n",
    "\n",
    "Going to try g4n.xlarge first, then g5.xlarge if its not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae75790",
   "metadata": {},
   "source": [
    "If I understand this right, the reason we need event-driven scaling is because an EC2 instance is needed per VM due to the block-y nature of processing a text-to-image request. OR is Kubernetes used on a global and local scale - I guess it makes sense if I'm paying for the EC2 anyway I might aswell use the full processing power, so Kubernetes will scale this LOCALLY first, and then if not enough to process the queue, it will then recruit more EC2s. But, KEDA can instead know how long the queue will be and get enough EC2s ready, which is more efficient. GPT agreed.\n",
    "\n",
    "BUT for simplicity I might not bother running Kubernetes on the actual EC2s themselves and just accept it is inefficient for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f769e2",
   "metadata": {},
   "source": [
    "KEY - will definitely have to shut down all these EC2s, too many are needed in total.\n",
    "I'll just have to record videos of it working, and make notes on how it all links together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68803d86",
   "metadata": {},
   "source": [
    "Just going to set up elastic IP for the frontend EC2 since that's the thing many other entities link to.\n",
    "\n",
    "Not going to use Stripe as I'm already learning interaction through an external API by using Firebase Auth, and it'll be long to actually test with money. Just going to modify database entries when needed.\n",
    "\n",
    "\n",
    "I'm also setting up the launch commands so when the EC2 instance starts it runs code to get the next.js server running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ad4c1",
   "metadata": {},
   "source": [
    "setting up the launch script:\n",
    "\n",
    "cd RollerAI/my-app\n",
    "npm run build\n",
    "npm start\n",
    "\n",
    "Ended up using pm2 as launch scripts require you to create the machine after defining the launch template:\n",
    "\n",
    "pm2 start npm --name \"roller-ai\" -- start\n",
    "\n",
    "pm2 save\n",
    "pm2 startup\n",
    "\n",
    "sudo env PATH=$PATH:/usr/bin /usr/lib/node_modules/pm2/bin/pm2 startup systemd -u ubuntu --hp /home/ubuntu\n",
    "\n",
    "That works now, next is to set up another EC2 as the worker bot and have them communicate via an API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cfa6d",
   "metadata": {},
   "source": [
    "setting up the worker bot:\n",
    "\n",
    "- chose 'deep learning ami neuron'\n",
    "- g4dn.xlarge to start with \n",
    "- had to request an increase in 'Running On-Demand G and VT instances' to 4 (It's not getting approved currently)\n",
    "\n",
    "OKAY I tried getting approval for spot and normal g-type EC2s and it's not getting approved.\n",
    "Based on GPT reccomendations I'm going to try runpod.io and GCP. I want to make sure there's still an aspect where the Redis/KEDA queue comes into use. \n",
    "\n",
    "Okay it's looking like I'll use runpod over GCP for simplicity, it's for short tasks so it's better, and no need for all the IAM/enterprise stuff on the google side.\n",
    "\n",
    "Just make sure you follow security practices if you use the 'community cloud' option which is where I think the spot pricing stuff is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e708f9",
   "metadata": {},
   "source": [
    "Right now the planned system diagram is looking something like this:\n",
    "\n",
    "+----------------+             +----------------+\n",
    "| Firebase Auth  | <---------> |   Credits DB   |\n",
    "+----------------+             +----------------+\n",
    "          ^                             ^\n",
    "          |                             |\n",
    "          v                             v\n",
    "     +---------------------------------------+\n",
    "     |              Frontend VM              |\n",
    "     +---------------------------------------+\n",
    "          ^                        |   ▲\n",
    "          |                        v   |\n",
    "          |              +---------------------+\n",
    "          |              |    Redis + KEDA     |\n",
    "          |              +---------------------+\n",
    "          |                        |\n",
    "          |                        v\n",
    "          |              +---------------------+\n",
    "          |              |       RunPod        |\n",
    "          |              |  +---------------+  |\n",
    "          |              |  |     Pod 1     |  |\n",
    "          |              |  +---------------+  |\n",
    "          |              |  |     Pod 2     |  |\n",
    "          |              |  +---------------+  |\n",
    "          |              |  |     Pod 3     |  |\n",
    "          |              |  +---------------+  |\n",
    "          |              +---------------------+\n",
    "          |                        |\n",
    "          |                        v\n",
    "          |              +---------------------+\n",
    "          +------------- |         S3          |\n",
    "                         +---------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd80c5",
   "metadata": {},
   "source": [
    "Runpod:\n",
    "\n",
    "- need to decide which of the services I will use, I know I will use the cheapest option which is the RTX A5000.\n",
    "- I will use 'Secure Cloud' not 'Community Cloud' as community just seems like an unreliable and inconsistent mess. The three options remaining are: pod spot; pod normal hourly; serverless\n",
    "\n",
    "So I've decided to use pod spot. The jobs are very short and I want to implement the try/except logic anyway. Also I'm not choosing serverless due to price and the fact that there'd be no point for the Redis/KEDA stuff. Going to pause this stuff and get Firebase/credits working first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa044950",
   "metadata": {},
   "source": [
    "Firebase + credits:\n",
    "\n",
    "Key for SQL - have a normal id as the primary as it makes SQL operations quicker, and use 'UNIQUE NOT NULL' for unique identifiers so your database maintains integrity.\n",
    "\n",
    "I initally thought once I get the token from Firebase it just gives me an ID I use to search in the database but you need a verifier before you allow it to access a DB, and in general it's not good practice to expose a database to the frontend. The SDK will live on the same server as the frontend server. I'll have to use pm2 to have the two systems working simultaneously.\n",
    "\n",
    "Using Amazon RDS to host a PostgreSQL database.\n",
    "\n",
    "ACTUALLY I realised I need to do authentication at the end because otherwise I can't test Redis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95dd4b",
   "metadata": {},
   "source": [
    "New system diagram:\n",
    "\n",
    "\n",
    "+----------------+             +----------------+\n",
    "| Firebase Auth  |             |   Credits DB   |\n",
    "+----------------+             +----------------+\n",
    "          ^                             ^\n",
    "          |                             |\n",
    "          v                             v\n",
    "     +---------------------------------------+\n",
    "     |              Frontend VM              |\n",
    "     |         --------------------          |\n",
    "     |           Firebase Auth SDK           |\n",
    "     +---------------------------------------+\n",
    "          ^                        |   ▲\n",
    "          |                        v   |\n",
    "          |              +---------------------+\n",
    "          |              |    Redis + KEDA     |\n",
    "          |              +---------------------+\n",
    "          |                        |\n",
    "          |                        v\n",
    "          |              +---------------------+\n",
    "          |              |       RunPod        |\n",
    "          |              |  +---------------+  |\n",
    "          |              |  |     Pod 1     |  |\n",
    "          |              |  +---------------+  |\n",
    "          |              |  |     Pod 2     |  |\n",
    "          |              |  +---------------+  |\n",
    "          |              |  |     Pod 3     |  |\n",
    "          |              |  +---------------+  |\n",
    "          |              +---------------------+\n",
    "          |                        |\n",
    "          |                        v\n",
    "          |              +---------------------+\n",
    "          +------------- |         S3          |\n",
    "                         +---------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fd2d7",
   "metadata": {},
   "source": [
    "Ok I reiterated to GPT I just want two pods, and a Python script was suggested instead. I'm going to try figure it out myself, because I want some type of hysterisis where it spins up the second one when queue is bigger than 4 but then only stops when queue is less than 2. It's better too since it will get me to MVP quicker. Also removed the arrow between Firebase Auth.\n",
    "\n",
    "Also I thought more about how I'll actually get the image displayed. My thought was that I generate a unique ID with the prompt, then I poll the s3 waiting for the image to be generated that has the id. But S3 URLs are random, it's costly to constantly poll s3, and there's no timeout/error strategy. \n",
    "\n",
    "After conversation with GPT the new strategy is runpod uploads to an s3 url, and sends a 'complete' signal back to the redis ec2. The front end is constantly polling the redis hash to check if the relevant id key value is complete, and when it is, it grabs the key which is the url, sends that url to the front end which loads the right image . \n",
    "\n",
    "So this means the arrow between runpod and the redis VM goes both ways now. Going to upgrade to a t3.medium just in case.\n",
    "\n",
    "New system diagram is below:\n",
    "\n",
    "+----------------+          +----------------+\n",
    "| Firebase Auth  |          |   Credits DB   |\n",
    "+----------------+          +----------------+\n",
    "          ^                          ^\n",
    "          |                          |\n",
    "          v                          v\n",
    "+---------------------------------------+\n",
    "|              Frontend VM              |\n",
    "|         --------------------          |\n",
    "|           Firebase Auth SDK      (EC2)|\n",
    "+---------------------------------------+\n",
    "     ^                        |   ^\n",
    "     |                        v   |\n",
    "     |    +------------------------------+\n",
    "     |    | Redis + Python script   (EC2)|\n",
    "     |    +------------------------------+\n",
    "     |                        |  ^\n",
    "     |                        v  |\n",
    "     |              +---------------------+\n",
    "     |              |       RunPod        |\n",
    "     v              |  +---------------+  |\n",
    "+------------+      |  |     Pod 1     |  |\n",
    "|     S3     |<-----|  +---------------+  |\n",
    "+------------+      |  |     Pod 2     |  |\n",
    "                    |  +---------------+  |\n",
    "                    +---------------------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ac9f2",
   "metadata": {},
   "source": [
    "This is the work sequence I'm going to follow, I'm pretty sure that my system diagram is set in stone now:\n",
    "\n",
    "- set up t3.medium for redis system with ElasticIP\n",
    "- work on getting string input with text box that sends to redis backend side\n",
    "- then get the API working so runpod sends to s3\n",
    "- create redis queue so multiple requests can happen at the same time\n",
    "- create redis hash and setup APIs and frontend code so image appears on frontend\n",
    "- add on your python script so tasks are balanced between two workers spun up as necessary with hysterisis\n",
    "- set up firebase auth so user has to log in before being able to request an image\n",
    "- add a credits system linked to RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38adb37",
   "metadata": {},
   "source": [
    "So the python pseudo-equivalent of pm2 is uvicorn:\n",
    "\n",
    "sudo nano /etc/systemd/system/fastapi.service\n",
    "inside it write this\n",
    "\n",
    "[Unit]\n",
    "Description=FastAPI app with Uvicorn\n",
    "After=network.target\n",
    "\n",
    "[Service]\n",
    "User=ubuntu\n",
    "WorkingDirectory=/home/ubuntu/RollerAI/redis_vm\n",
    "ExecStart=/usr/bin/env uvicorn scaler:app --host 0.0.0.0 --port 8000\n",
    "Restart=always\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "\n",
    "sudo systemctl daemon-reexec\n",
    "sudo systemctl daemon-reload\n",
    "sudo systemctl enable fastapi.service\n",
    "sudo systemctl start fastapi.service\n",
    "\n",
    "adding CORS to try get communication working - this wasn't the issue. GPT suggested using console on chrome, I see it's because although the frontend is in https, backened is http, so I get 'mixed content' error"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
